# 🧪 loadtest-e2e

부하 테스트 후속 프로젝트입니다.  
E2E 테스트 코드 분리와 부하 테스트 자동화 및 결과 분석 구조화에 중점을 두었습니다.

---

## 📌 개요

### 🔍 테스트 목적 및 범위

본 테스트는 **웹소켓 기반 채팅 시스템**의 부하 상황에서의 안정성과 성능을 검증하기 위해 실시되었습니다.  
특히 다음 시나리오에 초점을 맞추어 시스템의 **한계와 병목 현상**을 파악하고자 했습니다.

### 🧱 테스트 대상 시스템

- **프론트엔드**: React  
- **백엔드**: Node.js + WebSocket  
- **주요 기능**: 사용자 접속 유지, 채팅방 입장 및 메시지 송수신

---

## 🧰 테스트 환경 및 도구

| 항목         | 내용                                       |
|--------------|--------------------------------------------|
| 테스트 도구  | [Artillery.io](https://www.artillery.io/) |
| 브라우저     | Headless (Playwright 사용)                |
| 인프라       | AWS Fargate (ECS 기반)                     |
| 리전         | ap-northeast-2 (서울)                      |
| 작업자 수    | 5~14개의 Fargate 작업자                    |

---

## 🎯 테스트 시나리오

### ✅ 접속 유지 시나리오

- 사용자는 서비스에 접속한 후 **약 5분 동안 연결을 유지**
- 전체 사용자의 **88~90%**를 차지

### ✅ 채팅방 시나리오

- 사용자가 채팅방에 입장하여 메시지를 주고받는 상호작용 수행 (**약 5분 동안 연결을 유지**)
- 전체 사용자의 **10~12%**를 차지

---

## 🚀 테스트 실행 커맨드

```bash
artillery run-fargate test-artillery.yaml \
  --region ap-northeast-2 \
  --cluster artilleryio-cluster \
  --count 15 \ 
  --cpu 8192 \
  --memory 16384 \
  --output ./artillery-results/result-$(date +%Y%m%d-%H%M%S).json
```
## 결과 분석
### 🛠️ 병목 현상 및 해결 전략

#### 1. Redis Engine CPU 과부하

- **문제**: 약 1000명의 동시 접속 테스트에서 Redis 요청 실패율이 높아짐
- **원인**: 가장 저사양 Redis 인스턴스 1개(t3.micro) 사용 시,  
  엔진 CPU 사용률이 **90% 이상**으로 급등 (CloudWatch 기준 1분 평균 80% 이상)
- **해결**:
  - Redis 구성 변경: **t3.small 기반 인스턴스 3개 샤드 구성**
  - 단일 노드 → **Redis Cluster 형태로 전환**하여 부하 분산 처리


#### 2. MongoDB CPU 병목

- **문제**: 메시지 수신량이 증가함에 따라 MongoDB의 응답 지연 발생
- **원인**: 초당 수천 건의 읽기 요청 집중 → CPU 사용률이 **100% 근접**
- **해결**:
  - MongoDB 인덱스 구조는 유지
  - 단순한 **스케일 업 적용** (예: 2 vCPU → 4 vCPU)
  - Replica 구성 없이 **단일 인스턴스 성능 확보**에 중점


#### 3. Playwright 브라우저 리소스 문제

- **문제**: 다수의 브라우저 인스턴스를 띄우면 테스트 서버 자원이 과도하게 소모됨
- **원인**: 브라우저 단위 실행 시 연결 유지에 필요한 리소스가 기하급수적으로 증가
- **해결**:
  - 하나의 브라우저에 **200개의 context**를 생성해 시뮬레이션
  - 이 수치는 테스트 인스턴스 (8 vCPU / 16GB RAM) 기준으로  
    안정적으로 연결 가능한 최대치로 판단됨
  - **LRU 방식**으로 오래된 context를 닫고 새 context를 생성하는 방식 구현

---
### 🔹 테스트 #1 : 1000명 동시 접속 유지 ( 테스트 성공율 99.9% )
세션 유지 기간 제외 결과를 그래프로 나타냄.
- **구성**:  
  - Node.js 인스턴스 5개  
  - MongoDB 인스턴스 1개  
  - Redis Sharding + 스케일업  
  - S3, CDN 구성 포함
- **부하 조건**:  
  - 초당 5명의 사용자 접속  
  - 30초간 누적 접속자 150명
  - 5분 유지
<img src="https://github.com/user-attachments/assets/a7931776-e1e9-4353-8d7a-2aa32a560363" alt="test3_histogram" width="600px" />

### 🔹 테스트 #2 : 3000명 동시 접속 유지 ( 테스트 성공율 98.5% )
세션 유지 기간 제외 결과를 그래프로 나타냄.
- **구성**:  
  - Node.js 인스턴스 15개  
  - MongoDB 인스턴스 1개 (스케일업 적용. t3.small -> t3.large)  
  - Redis Sharding 구성  
  - S3, CDN 포함
- **부하 조건**:  
  - 초당 15명의 사용자 접속  
  - 30초간 누적 접속자 450명
<img src="https://github.com/user-attachments/assets/e14bf29d-7bb6-4a89-aae6-e6b7cd68475c" alt="test1_histogram" width="600px" />

